
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    /* margin: 16px 0px; */
    margin: 0px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}



/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:title" content="Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new"> 
                <a href="https://www.linkedin.com/in/shivam-jain-822198204/">Shivam Jain</a>,
                <a href="https://www.linkedin.com/in/sachit-sachdeva-559769214/">Sachit Sachdeva</a>,
                <a href="https://www.cse.iitd.ac.in/~rohanpaul/">Rohan Paul</a>
            </div>
        </center>
        <!-- <center> -->
        <div class="affil-row">
            <div class="venue text-center">Indian Institute of Technology, Delhi</div>
        </div>
        </center>
        <!-- <br> -->
        <center>
        <div class="affil-row">
            <div class="venue text-center"><b>CoRL 2025</b></div>
        </div>
        </center>
        <br>

        <div style="clear: both">
            <!-- <div class="paper-btn-parent">
                <a class="paper-btn" href="">
                        <span class="material-icons"> description </span> 
                    Paper
                </a> -->
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://openreview.net/pdf?id=fBRqCMqVyS">
                <span class="material-icons"> description </span> 
                arXiv
            </a>
            <a class="paper-btn" href="https://github.com/shivam89jain/Franka-LHT-Simulator" style="width:150px;">
            <span class="material-icons"> aspect_ratio </span>
            FrankaLHT
            </a>
            <a class="paper-btn" href="https://huggingface.co/shivam89jain/Trained-Checkpoints-SGPT">
                <span class="material-icons"> cloud_download </span>
                Data
            </a>
            <a class="paper-btn" href="https://github.com/shivam89jain/SGPT-long-horizon-imitation">
                <span class="material-icons"> code </span>
                Code
            </a>
        </div>
    </div>

    <br><br>

    <div class="embed-container">
        <!-- <iframe width="640" height="360" -->
        <iframe width="560" height="315"
        src="./mfiles/Overview.mp4" 
        frameborder="0" allowfullscreen></iframe>
    </div>
    <style>
        .embed-container {
        position: relative;
        padding-bottom: 56.25%;
        height: 0;
        overflow: hidden;
        max-width: 100%;
        }
        .embed-container iframe,
        .embed-container object,
        .embed-container embed {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        }
    </style>
    
    <section id="abstract"/>
        <!-- <hr> -->
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                Imitation-based policy training for long-horizon manipulation tasks involving multi-step object interactions is often susceptible to compounding action errors. Contemporary methods discover semantic subgoals embedded within the overall task, decomposing the overall task into tractable shorter-horizon goal-conditioned policy learning. However, policy deployment requires iteratively estimating which subgoal is being pursued and when it is achieved. We observe the brittleness of conventional heuristic-based approaches (ad hoc threshold-based), particularly for long-horizon imitation, since pursuing an incorrect subgoal can lead the robot policy to experience out-of-distribution states. In this work, we introduce two policy architectures for modeling subgoal transitions within a policy learning loop for long-horizon tasks. The first model autoregressively predicts the likelihood of the next subgoal transition, while the second uses cross-attention (via a transformer-based architecture) and implicitly models smooth and continuous transitions. We evaluate our models on 25 simulated tasks on Franka Kitchen, 6 real-world tabletop tasks, and 18 simulated tasks on a new corpus (Franka-Long Horizon Tasks, LHT) focused on tasks with rich object interactions over long episode lengths. Experimental results show significant improvements in learning efficacy, task success rates, and generalization to out-of-distribution settings â€” extending horizon lengths for imitating manipulation tasks from long to long(er).
            </p>
        </div>
    </section>
    <section id="method"/>

        <!-- <br><br>
        <center>
            <div class="mx-auto">
                <img class="card-img-top" src="./mfiles/images/intro.jpeg" style="width:900">
            </div>
        </center> -->

        <!-- Video style -->
        <style>
            select {
                padding: 10px;
                font-size: 16px;
                border-radius: 5px;
            }
        
            video {
                display: block;
                max-width: 500px;
                margin-top: 20px;
            }
        </style>
    </section>

    <section>
        <hr>
        <h2>Proposed Architecture</h2>
        <p>
            SGPT architcture for modeling continuous subgoal transitions. Cross-attention over subgoals allows the model to capture subgoal dependencies, enabling fluid subgoal transitions and seamless execution of long-horizon tasks.
        </p>
        <br><br>
        <center>
            <div class="mx-auto">
                <img class="card-img-top" src="./mfiles/images/sgpt.png" style="width:600">
            </div>
        </center>
        <br><br>

        <hr>
        <h2>Environments</h2>
        <p>
            Environments used in our experiments along with the average episode lengths (figure not to scale) of demonstration trajectories, indicating increasing task horizons.
        </p>
        <br><br>
        <center>
            <div class="mx-auto">
                <img class="card-img-top" src="./mfiles/images/env.png" style="width:900">
            </div>
        </center>
        <br><br>

        <hr>
        <h2>FrankaLHT Simulation Environment</h2>
        <p>
            The FrankaLHT (Long-Horizon Tasks) environment is built on the PyBullet physics engine and provides a challenging benchmark for evaluating policies on temporally extended manipulation tasks. The environment includes 180 demonstration trajectories collected via keyboard teleoperation, with the Franka Panda robot operating at a control frequency of 50 Hz.
        </p>
        <br><br>
        <center>
            <div class="mx-auto">
                <img class="card-img-top" src="./mfiles/images/initial_scenes.png" style="width:600">
            </div>
        </center>
        <br><br>

        <hr>
        <h2>Performance Comparison</h2>
        <p>
            Performance comparison of all architectures across both the simulation environments(top) and the real world environment(bottom). For MLP and GPT, we report results using the subgoal transition threshold that yields the best InD performance. While most models perform well in the simpler Franka Kitchen setting, their performance drops significantly in the more challenging FrankaLHT environment. SGPT consistently outperforms all baselines across both settings (S: Success rate, C: Completion rate).
        </p>
        <br><br>
        <center>
            <div class="mx-auto">
                <img class="card-img-top" src="./mfiles/images/cmp2.png" style="width:800">
            </div>
            <div class="mx-auto">
                <img class="card-img-top" src="./mfiles/images/cmp1.png" style="width:500">
            </div>
        </center>
        <br><br>

        <hr>
        <h2>FrankaLHT policy Rollouts</h2>
        <br>
        <h3>Successful Rollouts</h3>
        <label for="video-select1-franka">Task:</label>
        <select id="video-select1-franka" onchange="displayVideo1Franka()">
            <option value="mfiles/franka_lht/success_1.mp4">pour water in glass</option>
            <option value="mfiles/franka_lht/success_2.mp4">food packing</option>
            <option value="mfiles/franka_lht/success_3.mp4">waste disposal</option>
            <option value="mfiles/franka_lht/success_4.mp4">bottle packing</option>
        </select>

        <div>
            <video id="selected-video1-franka" controls>
                Your browser does not support the video tag.
            </video>
        </div>

        <script>
            function displayVideo1Franka() {
                var selectElement = document.getElementById("video-select1-franka");
                var selectedVideo = selectElement.options[selectElement.selectedIndex].value;
                var videoElement = document.getElementById("selected-video1-franka");
                videoElement.src = selectedVideo;
            }
            displayVideo1Franka();
        </script>
        <br><br>

        <h3>Failed Rollouts</h3>
        <label for="video-select2-franka">Task:</label>
        <select id="video-select2-franka" onchange="displayVideo2Franka()">
            <option value="mfiles/franka_lht/fail_1.mp4">waste disposal</option>
            <option value="mfiles/franka_lht/fail_2.mp4">bottle packing</option>
            <option value="mfiles/franka_lht/fail_3.mp4">carrot in basket</option>
        </select>

        <div>
            <video id="selected-video2-franka" controls>
                Your browser does not support the video tag.
            </video>
        </div>

        <script>
            function displayVideo2Franka() {
                var selectElement = document.getElementById("video-select2-franka");
                var selectedVideo = selectElement.options[selectElement.selectedIndex].value;
                var videoElement = document.getElementById("selected-video2-franka");
                videoElement.src = selectedVideo;
            }
            displayVideo2Franka();
        </script>
        <br><br>

        <hr>
        <h2>Real World policy Rollouts</h2>
        <br>
        <h3>Successful Rollouts</h3>
        <label for="video-select1-real">Task:</label>
        <select id="video-select1-real" onchange="displayVideo1Real()">
            <option value="mfiles/real_world/success_1.mp4">can and tray disposal</option>
            <option value="mfiles/real_world/success_2.mp4">sunscreen and tray disposal</option>
            <option value="mfiles/real_world/success_3.mp4">vegetable in basket</option>
        </select>

        <div>
            <video id="selected-video1-real" controls>
                Your browser does not support the video tag.
            </video>
        </div>

        <script>
            function displayVideo1Real() {
                var selectElement = document.getElementById("video-select1-real");
                var selectedVideo = selectElement.options[selectElement.selectedIndex].value;
                var videoElement = document.getElementById("selected-video1-real");
                videoElement.src = selectedVideo;
            }
            displayVideo1Real();
        </script>
        <br><br>

        <h3>Failed Rollouts</h3>
        <label for="video-select2-real">Task:</label>
        <select id="video-select2-real" onchange="displayVideo2Real()">
            <option value="mfiles/real_world/fail_1.mp4">can and tray disposal</option>
            <option value="mfiles/real_world/fail_2.mp4">jar disposal</option>
            <option value="mfiles/real_world/fail_3.mp4">fruits in basket</option>
        </select>

        <div>
            <video id="selected-video2-real" controls>
                Your browser does not support the video tag.
            </video>
        </div>

        <script>
            function displayVideo2Real() {
                var selectElement = document.getElementById("video-select2-real");
                var selectedVideo = selectElement.options[selectElement.selectedIndex].value;
                var videoElement = document.getElementById("selected-video2-real");
                videoElement.src = selectedVideo;
            }
            displayVideo2Real();
        </script>
        <br><br>

    </section>

    <section id="paper">
        <h2>Bibtex</h2>
        <div class="page-body"><pre id="ad6975be-3353-467d-ae48-6313d767ffa6" class="code"><code>
            @inproceedings{
                jain2025enabling,
                title={Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions},
                author={Shivam Jain and Sachit Sachdeva and Rohan Paul},
                booktitle={9th Annual Conference on Robot Learning},
                year={2025},
                url={https://openreview.net/forum?id=fBRqCMqVyS}
                }
        </code></pre><p id="1a3aa306-c4b8-4872-8fb0-411495c73d55" class="">
        </p></div>

    </section>
   

    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
    </section>
    


</div>
</body>
</html>
